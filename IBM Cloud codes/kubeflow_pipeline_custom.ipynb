{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Digit Recognizer Kubeflow Pipeline\n",
    "\n",
    "In this [Kaggle competition](https://www.kaggle.com/competitions/digit-recognizer/overview) \n",
    "\n",
    ">MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\n",
    "\n",
    ">In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Install relevant libraries\n",
    "\n",
    "\n",
    ">Update pip `pip install --user --upgrade pip`\n",
    "\n",
    ">Install and upgrade kubeflow sdk `pip install kfp --upgrade --user --quiet`\n",
    "\n",
    "You may need to restart your notebook kernel after installing the kfp sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "38y13drotnXK",
    "outputId": "61184254-57cb-4f29-c0e5-dba30df0c914",
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "# !virtualenv venv\n",
    "# !pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6a8V8LN9ttJT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp-tekton==1.2.0 in /opt/conda/lib/python3.8/site-packages (1.2.0)\n",
      "Collecting kfp<1.8.12,>=1.8.10\n",
      "  Using cached kfp-1.8.11-py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (5.4.1)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.13 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.1.13)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (3.0.1)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.13)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (7.1.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.4.1)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.11.0)\n",
      "Requirement already satisfied: google-cloud-storage<2,>=1.20.0 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.44.0)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.2.13)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.1.10)\n",
      "Collecting cloudpickle<3,>=2.0.0\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.31.0)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.4.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.8.9)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.6.0)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.9.1)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.12.10)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.9.0)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (12.0.1)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (3.2.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.8/site-packages (from kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (3.17.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from absl-py<2,>=0.9->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.8/site-packages (from Deprecated<2,>=1.2.7->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from fire<1,>=0.3.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.1.0)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.8/site-packages (from google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.7.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.8/site-packages (from google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.8/site-packages (from google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.20.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.25.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.55.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (4.2.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<2,>=1.20.0->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.2.3)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.8/site-packages (from google-cloud-storage<2,>=1.20.0->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.3.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.8/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage<2,>=1.20.0->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.8/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.4.7)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema<4,>=3.0.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema<4,>=3.0.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (21.2.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.26.5)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.8.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.8/site-packages (from kubernetes<19,>=8.0.0->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.8/site-packages (from kubernetes<19,>=8.0.0->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client<2,>=1.7.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (4.0.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from strip-hints<1,>=0.1.8->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (0.36.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp<1.8.12,>=1.8.10->kfp-tekton==1.2.0) (3.1.1)\n",
      "Installing collected packages: cloudpickle, kfp\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 1.6.0\n",
      "    Uninstalling cloudpickle-1.6.0:\n",
      "      Successfully uninstalled cloudpickle-1.6.0\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 1.8.4\n",
      "    Uninstalling kfp-1.8.4:\n",
      "      Successfully uninstalled kfp-1.8.4\n",
      "Successfully installed cloudpickle-2.0.0 kfp-1.8.11\n"
     ]
    }
   ],
   "source": [
    "# !source venv/bin/activate\n",
    "!pip install kfp-tekton==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "taqx2u69tnXS",
    "outputId": "32ec75b1-7d1e-434e-8834-aa841fecd4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 1.8.11\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: https://github.com/kubeflow/pipelines\n",
      "Author: The Kubeflow Authors\n",
      "Author-email: None\n",
      "License: UNKNOWN\n",
      "Location: /opt/conda/lib/python3.8/site-packages\n",
      "Requires: requests-toolbelt, google-auth, jsonschema, google-api-python-client, Deprecated, typer, absl-py, google-cloud-storage, click, kubernetes, kfp-server-api, PyYAML, pydantic, fire, typing-extensions, protobuf, strip-hints, docstring-parser, cloudpickle, tabulate, uritemplate, kfp-pipeline-spec\n",
      "Required-by: kfp-tekton\n",
      "Name: kfp-tekton\n",
      "Version: 1.2.0\n",
      "Summary: Tekton Compiler for Kubeflow Pipelines\n",
      "Home-page: https://github.com/kubeflow/kfp-tekton/\n",
      "Author: kubeflow.org\n",
      "Author-email: None\n",
      "License: Apache 2.0\n",
      "Location: /opt/conda/lib/python3.8/site-packages\n",
      "Requires: kfp\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# confirm the kfp sdk\n",
    "!pip show kfp\n",
    "!pip show kfp_tekton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B2CjdRcuRgT"
   },
   "source": [
    "## Import kubeflow pipeline libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_8P4-rCDtnXT"
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp_tekton import TektonClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWPLyw_DuzSl"
   },
   "source": [
    "## Kubeflow pipeline component creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98g9LoIcuaPB"
   },
   "source": [
    "Component 1: Download the digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lGK3hlXdtnXV"
   },
   "outputs": [],
   "source": [
    "# download data step\n",
    "def download_data(download_link: str, data_path: OutputPath(str)):\n",
    "    import zipfile\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"wget\"])\n",
    "    import wget\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    # download files\n",
    "    wget.download(download_link.format(file='train'), f'{data_path}/train_csv.zip')\n",
    "    wget.download(download_link.format(file='test'), f'{data_path}/test_csv.zip')\n",
    "    \n",
    "    with zipfile.ZipFile(f\"{data_path}/train_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "        \n",
    "    with zipfile.ZipFile(f\"{data_path}/test_csv.zip\",\"r\") as zip_ref:\n",
    "        zip_ref.extractall(data_path)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9Yhdk3xudcn"
   },
   "source": [
    "Component 2: load the digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oEdsQpH2tnXX"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "def load_data(data_path: InputPath(str), \n",
    "              load_data_path: OutputPath(str)):\n",
    "    \n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "\n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    train_data_path = data_path + '/train.csv'\n",
    "    test_data_path = data_path + '/test.csv'\n",
    "\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    \n",
    "    ntrain = train_df.shape[0]\n",
    "    ntest = test_df.shape[0]\n",
    "    all_data = pd.concat((train_df, test_df)).reset_index(drop=True)\n",
    "    print(\"all_data size is : {}\".format(all_data.shape))\n",
    "    \n",
    "    os.makedirs(load_data_path, exist_ok = True)\n",
    "    \n",
    "    with open(f'{load_data_path}/all_data', 'wb') as f:\n",
    "        pickle.dump((ntrain, all_data), f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Component 3: Preprocess the digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "\n",
    "def preprocess_data(load_data_path: InputPath(str), \n",
    "                    preprocess_data_path: OutputPath(str)):\n",
    "    \n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    import os, pickle;\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #loading the train data\n",
    "    with open(f'{load_data_path}/all_data', 'rb') as f:\n",
    "        ntrain, all_data = pickle.load(f)\n",
    "    \n",
    "    # split features and label\n",
    "    all_data_X = all_data.drop('label', axis=1)\n",
    "    all_data_y = all_data.label\n",
    "    \n",
    "    # Reshape image in 3 dimensions (height = 28px, width = 28px , channel = 1)\n",
    "    all_data_X = all_data_X.values.reshape(-1,28,28,1)\n",
    "\n",
    "    # Normalize the data\n",
    "    all_data_X = all_data_X / 255.0\n",
    "    \n",
    "    #Get the new dataset\n",
    "    X = all_data_X[:ntrain].copy()\n",
    "    y = all_data_y[:ntrain].copy()\n",
    "    \n",
    "    # split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(preprocess_data_path, exist_ok = True)\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the modelling component.\n",
    "    with open(f'{preprocess_data_path}/train', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "        \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{preprocess_data_path}/test', 'wb') as f:\n",
    "        pickle.dump((X_test,  y_test), f)\n",
    "    \n",
    "    return(print('Done!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Component 4: ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8KGMGMEmtnXZ"
   },
   "outputs": [],
   "source": [
    "def modeling(preprocess_data_path: InputPath(str), \n",
    "            model_path: OutputPath(str)):\n",
    "    \n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','tensorflow'])\n",
    "    import os, pickle;\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras, optimizers\n",
    "    from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "    from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "    from tensorflow.keras import layers\n",
    "\n",
    "    #loading the train data\n",
    "    with open(f'{preprocess_data_path}/train', 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "        \n",
    "    # Separate the X_train from y_train.\n",
    "    X_train, y_train = train_data\n",
    "    \n",
    "    #initializing the classifier model with its input, hidden and output layers\n",
    "    hidden_dim1=56\n",
    "    hidden_dim2=100\n",
    "    DROPOUT=0.5\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters = hidden_dim1, kernel_size = (5,5),padding = 'Same', \n",
    "                         activation ='relu'),\n",
    "            tf.keras.layers.Dropout(DROPOUT),\n",
    "            tf.keras.layers.Conv2D(filters = hidden_dim2, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'),\n",
    "            tf.keras.layers.Dropout(DROPOUT),\n",
    "            tf.keras.layers.Conv2D(filters = hidden_dim2, kernel_size = (3,3),padding = 'Same', \n",
    "                         activation ='relu'),\n",
    "            tf.keras.layers.Dropout(DROPOUT),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10, activation = \"softmax\")\n",
    "            ])\n",
    "\n",
    "    model.build(input_shape=(None,28,28,1))\n",
    "    \n",
    "    #Compiling the classifier model with Adam optimizer\n",
    "    model.compile(optimizers.Adam(learning_rate=0.001), \n",
    "              loss=SparseCategoricalCrossentropy(), \n",
    "              metrics=SparseCategoricalAccuracy(name='accuracy'))\n",
    "\n",
    "    # model fitting\n",
    "    history = model.fit(np.array(X_train), np.array(y_train),\n",
    "              validation_split=.1, epochs=1, batch_size=64)\n",
    "    \n",
    "    #loading the X_test and y_test\n",
    "    with open(f'{preprocess_data_path}/test', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_test from y_test.\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Evaluate the model and print the results\n",
    "    test_loss, test_acc = model.evaluate(np.array(X_test),  np.array(y_test), verbose=0)\n",
    "    print(\"Test_loss: {}, Test_accuracy: {} \".format(test_loss,test_acc))\n",
    "    \n",
    "    #creating the preprocess directory\n",
    "    os.makedirs(model_path, exist_ok = True)\n",
    "      \n",
    "    #saving the model\n",
    "    model.save(f'{model_path}/model.h5')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwXJuoHQui3d"
   },
   "source": [
    "Component 5: Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model_path: InputPath(str), \n",
    "                preprocess_data_path: InputPath(str), \n",
    "                mlpipeline_ui_metadata_path: OutputPath(str)) -> NamedTuple('conf_m_result', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    \n",
    "    # import Library\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','scikit-learn'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','pandas'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install','tensorflow'])\n",
    "    import pickle, json;\n",
    "    import pandas as  pd\n",
    "    import numpy as np\n",
    "    from collections import namedtuple\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    #loading the X_test and y_test\n",
    "    with open(f'{preprocess_data_path}/test', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_test from y_test.\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    #loading the model\n",
    "    model = load_model(f'{model_path}/model.h5')\n",
    "    \n",
    "    # prediction\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    \n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    vocab = list(np.unique(y_test))\n",
    "    \n",
    "    # confusion_matrix pair dataset \n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append((vocab[target_index], vocab[predicted_index], count))\n",
    "    \n",
    "    # convert confusion_matrix pair dataset to dataframe\n",
    "    df = pd.DataFrame(data,columns=['target','predicted','count'])\n",
    "    \n",
    "    # change 'target', 'predicted' to integer strings\n",
    "    df[['target', 'predicted']] = (df[['target', 'predicted']].astype(int)).astype(str)\n",
    "    \n",
    "    # create kubeflow metric metadata for UI\n",
    "    metadata = {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"confusion_matrix\",\n",
    "                \"format\": \"csv\",\n",
    "                \"schema\": [\n",
    "                    {\n",
    "                        \"name\": \"target\",\n",
    "                        \"type\": \"CATEGORY\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"predicted\",\n",
    "                        \"type\": \"CATEGORY\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"count\",\n",
    "                        \"type\": \"NUMBER\"\n",
    "                    }\n",
    "                ],\n",
    "                \"source\": df.to_csv(header=False, index=False),\n",
    "                \"storage\": \"inline\",\n",
    "                \"labels\": [\n",
    "                    \"0\",\n",
    "                    \"1\",\n",
    "                    \"2\",\n",
    "                    \"3\",\n",
    "                    \"4\",\n",
    "                    \"5\",\n",
    "                    \"6\",\n",
    "                    \"7\",\n",
    "                    \"8\",\n",
    "                    \"9\",\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n",
    "        json.dump(metadata, metadata_file)\n",
    "\n",
    "    conf_m_result = namedtuple('conf_m_result', ['mlpipeline_ui_metadata'])\n",
    "    \n",
    "    return conf_m_result(json.dumps(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create light weight components\n",
    "download_op = comp.func_to_container_op(download_data,base_image=\"python:3.7.1\")\n",
    "load_op = comp.func_to_container_op(load_data,base_image=\"python:3.7.1\")\n",
    "preprocess_op = comp.func_to_container_op(preprocess_data,base_image=\"python:3.7.1\")\n",
    "modeling_op = comp.func_to_container_op(modeling, base_image=\"tensorflow/tensorflow:latest\")\n",
    "predict_op = comp.func_to_container_op(prediction, base_image=\"tensorflow/tensorflow:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create kubeflow pipeline components from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3bNFpBOuuG-"
   },
   "source": [
    "## Kubeflow pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVbUms_ptnXc",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ECZRaIgCtnXd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "@dsl.pipeline(name=\"pipeline-custom-pytorch\", \n",
    "              description=\"Performs Preprocessing, training and prediction of digits\")\n",
    "def custom_pipeline(download_link = 'https://github.com/josepholaide/KfaaS/blob/main/kale/data/{file}.csv.zip?raw=true',\n",
    "                data_path = '/mnt',\n",
    "                load_data_path = 'load',\n",
    "                preprocess_data_path = 'preprocess',\n",
    "                model_path = 'model',\n",
    "                            ):\n",
    "\n",
    "\n",
    "    # Create download container.\n",
    "    download_container = download_op(download_link)\n",
    "    load_container = load_op(download_container.output)\n",
    "    preprocess_container = preprocess_op(load_container.output)\n",
    "    modeling_container = modeling_op(preprocess_container.output)\n",
    "    predict_container = predict_op(modeling_container.output, preprocess_container.output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace download_link with the repo link where the data is stored https:github-repo/data-dir/{file}.csv.zip?raw=true\n",
    "download_link = 'https://github.com/josepholaide/KfaaS/blob/main/kale/data/{file}.csv.zip?raw=true'\n",
    "data_path = '/mnt'\n",
    "load_data_path = 'load'\n",
    "preprocess_data_path = 'preprocess'\n",
    "model_path = 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Jq2M3chhtnXd",
    "outputId": "cd75c395-f0d1-415f-8205-9ea23d52fdb5"
   },
   "outputs": [],
   "source": [
    "pipeline_func = custom_pipeline\n",
    "\n",
    "experiment_name = 'custom_pipeline'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {'download_link': download_link,\n",
    "             'data_path': data_path,\n",
    "             'load_data_path': load_data_path,\n",
    "             'preprocess_data_path': preprocess_data_path,\n",
    "             'model_path': model_path}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  custom_pipeline.zip\n",
      "  inflating: custom_pipeline/pipeline.yaml  \n"
     ]
    }
   ],
   "source": [
    "!unzip custom_pipeline.zip -d custom_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiments': [{'created_at': datetime.datetime(2022, 5, 3, 2, 39, 57, tzinfo=tzlocal()),\n",
      "                  'description': None,\n",
      "                  'id': 'e9f19212-bcf4-4a14-8d24-1e6536a2a96a',\n",
      "                  'name': 'Default',\n",
      "                  'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
      "                                                   'type': 'NAMESPACE'},\n",
      "                                           'name': None,\n",
      "                                           'relationship': 'OWNER'}],\n",
      "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2022, 5, 3, 2, 49, 45, tzinfo=tzlocal()),\n",
      "                  'description': None,\n",
      "                  'id': 'cea437c0-a8db-44bd-9d29-34ccb1be518a',\n",
      "                  'name': 'Demo Experiments',\n",
      "                  'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
      "                                                   'type': 'NAMESPACE'},\n",
      "                                           'name': None,\n",
      "                                           'relationship': 'OWNER'}],\n",
      "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2022, 5, 4, 22, 3, 8, tzinfo=tzlocal()),\n",
      "                  'description': None,\n",
      "                  'id': '22b5c8e8-a8ff-4758-b501-1a9c95bd45f7',\n",
      "                  'name': 'Test E2E Experiments',\n",
      "                  'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
      "                                                   'type': 'NAMESPACE'},\n",
      "                                           'name': None,\n",
      "                                           'relationship': 'OWNER'}],\n",
      "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2022, 5, 5, 4, 0, 55, tzinfo=tzlocal()),\n",
      "                  'description': None,\n",
      "                  'id': '79303ea8-a8dd-40f5-b620-c22d98f2aa40',\n",
      "                  'name': 'Test E2E Experiments - W/O Katib',\n",
      "                  'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
      "                                                   'type': 'NAMESPACE'},\n",
      "                                           'name': None,\n",
      "                                           'relationship': 'OWNER'}],\n",
      "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2022, 5, 5, 4, 14, 47, tzinfo=tzlocal()),\n",
      "                  'description': None,\n",
      "                  'id': '7ad8875d-a9c5-4582-ab9c-705d41818008',\n",
      "                  'name': 'Test E2E Experiments - Bayesian',\n",
      "                  'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
      "                                                   'type': 'NAMESPACE'},\n",
      "                                           'name': None,\n",
      "                                           'relationship': 'OWNER'}],\n",
      "                  'storage_state': 'STORAGESTATE_AVAILABLE'},\n",
      "                 {'created_at': datetime.datetime(2022, 5, 5, 4, 21, 49, tzinfo=tzlocal()),\n",
      "                  'description': None,\n",
      "                  'id': 'dc96c086-936f-437b-9788-c91256541fbe',\n",
      "                  'name': 'Test E2E Experiments - Grid',\n",
      "                  'resource_references': [{'key': {'id': 'kubeflow-user-example-com',\n",
      "                                                   'type': 'NAMESPACE'},\n",
      "                                           'name': None,\n",
      "                                           'relationship': 'OWNER'}],\n",
      "                  'storage_state': 'STORAGESTATE_AVAILABLE'}],\n",
      " 'next_page_token': None,\n",
      " 'total_size': 6}\n"
     ]
    }
   ],
   "source": [
    "from kfp_tekton import TektonClient\n",
    "\n",
    "KUBEFLOW_PUBLIC_ENDPOINT_URL = 'https://kubeflow-cml-group-projec-4f27b99c6360f285c2c732f9adc614f1-0003.us-east.containers.appdomain.cloud'\n",
    "KUBEFLOW_PROFILE_NAME = f'kubeflow-user-example-com'\n",
    "SESSION_COOKIE = f'authservice_session=MTY1MTcyMjM1NHxOd3dBTkU5U1VqSlNUVWRWTXpWVlRqZEhTemRMV1VOYVNFeFBXRFZYVFRSUU5sRkVXRXhKUXpKYVVUTkhXak5FU1RWWlZrcERUMUU9fKvYlgJLPelGI4L4q0361S30P-PeZOY3BRSMvYQt3rJM'\n",
    "\n",
    "client = TektonClient(host=f'{KUBEFLOW_PUBLIC_ENDPOINT_URL}/pipeline',\n",
    "                     cookies=SESSION_COOKIE)\n",
    "\n",
    "experiments = client.list_experiments(namespace=KUBEFLOW_PROFILE_NAME)\n",
    "print(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIMENT_NAME = 'Experiments using Custom Python Code + Pipeline'\n",
    "experiment = client.create_experiment(name=EXPERIMENT_NAME, namespace=KUBEFLOW_PROFILE_NAME)\n",
    "run = client.run_pipeline(experiment.id, 'pipeline-custom-pytorch', 'custom_pipeline/pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "kubepipe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "autosnapshot": true,
   "docker_image": "gcr.io/arrikto/jupyter-kale-py36@sha256:dd3f92ca66b46d247e4b9b6a9d84ffbb368646263c2e3909473c3b851f3fe198",
   "experiment": {
    "id": "6f6c9b81-54e3-414b-974a-6fe8b445a59e",
    "name": "digit_recognize_lightweight"
   },
   "experiment_name": "digit_recognize_lightweight",
   "katib_metadata": {
    "algorithm": {
     "algorithmName": "grid"
    },
    "maxFailedTrialCount": 3,
    "maxTrialCount": 12,
    "objective": {
     "objectiveMetricName": "",
     "type": "minimize"
    },
    "parallelTrialCount": 3,
    "parameters": []
   },
   "katib_run": false,
   "pipeline_description": "Performs Preprocessing, training and prediction of digits",
   "pipeline_name": "digit-recognizer-kfp",
   "snapshot_volumes": true,
   "steps_defaults": [
    "label:access-ml-pipeline:true",
    "label:access-rok:true"
   ],
   "volume_access_mode": "rwm",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "arikkto-workspace-7xzjm",
     "size": 5,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
